{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:19:33.808461Z",
     "start_time": "2024-08-29T00:19:27.070412Z"
    },
    "id": "uep96yWDqBq1"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import PyPDF2\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLenLtoyj7d_"
   },
   "source": [
    "# Coleta de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:19:34.409839Z",
     "start_time": "2024-08-29T00:19:34.405412Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1So0ez1UiLja",
    "outputId": "ceed1e57-1d01-4308-8474-97c924df2ce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta 'dataset' já existe.\n",
      "Pasta 'dataset/prices' já existe.\n",
      "Pasta 'dataset/prices_processed' já existe.\n"
     ]
    }
   ],
   "source": [
    "folders = [\"dataset\", \"dataset/prices\", \"dataset/prices_processed\"]\n",
    "\n",
    "# Verifica se as pastas existem, se não, cria-as\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Pasta '{folder}' foi criada.\")\n",
    "    else:\n",
    "        print(f\"Pasta '{folder}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "eHjgdyxtqEgX",
    "outputId": "39f959b0-8b14-4b7d-b9a1-1b352933d58a"
   },
   "outputs": [],
   "source": [
    "# API YahooFinance para baixar os dados historicos\n",
    "def HistoricalData(ticker, startDate, endDate, path2save = ''):\n",
    "\n",
    "  \"\"\"\n",
    "  ticker: Simbolo ação. Ex: VALE\n",
    "  startDate: Data inicial. Ex: 2010-01-01\n",
    "  endDate: Data final. Ex: 2020-12-31\n",
    "  path2save: Caminho para salvar o dataframe.\n",
    "  \"\"\"\n",
    "  data = yf.download(ticker, start=startDate, end=endDate)\n",
    "  df = pd.DataFrame(data)\n",
    "\n",
    "  if path2save != '':\n",
    "    df.to_csv(path2save)\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "#dados = HistoricalData(\"ANIM3.SA\", startDate='2010-01-01', endDate='2023-01-01')\n",
    "#dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:19:39.303697Z",
     "start_time": "2024-08-29T00:19:39.295354Z"
    },
    "id": "_97em83mqMFQ"
   },
   "outputs": [],
   "source": [
    "# Criando ferramenta para adicionar novos campos para o dataframe\n",
    "def catalog_return(row, x, name_return):\n",
    "    if row[name_return] > x * row[f'Cumulative_std_{name_return}']:\n",
    "        return 1\n",
    "    elif row[name_return] < -x * row[f'Cumulative_std_{name_return}']:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class DataProcessing:\n",
    "    def __init__(self, data):\n",
    "        self.dataframe = data\n",
    "        self.dataframe['Date'] = pd.to_datetime(self.dataframe['Date'])\n",
    "        self.dataframe = self.dataframe.sort_values(by='Date')    \n",
    "        self.dataframe['WClose'] = self.dataframe['Adj Close'] * self.dataframe['Volume'] / self.dataframe['Volume'].mean()\n",
    "        self.dataframe['WClose'] = self.dataframe['WClose'].rolling(window=10).mean()\n",
    "        \n",
    "    def get_by_date_range(self, start_date, end_date):\n",
    "        mask = ((self.dataframe['Date'] >= start_date) & (self.dataframe['Date'] <= end_date))\n",
    "        return self.dataframe.loc[mask]\n",
    "\n",
    "    def get_by_date(self, date):\n",
    "        return self.dataframe.loc[(self.dataframe['Date'] == date)]\n",
    "\n",
    "    def create_return_by_period(self, name_return, period, column_name = 'Adj Close', remove_nan=False):\n",
    "        self.dataframe[f'{name_return}'] = np.log(\n",
    "            self.dataframe[column_name] / self.dataframe[column_name].shift(period))\n",
    "        if remove_nan:\n",
    "            self.dataframe = self.dataframe.dropna()\n",
    "\n",
    "    def create_cumulative_std(self, name_return):\n",
    "        self.dataframe[f'Cumulative_std_{name_return}'] = self.dataframe[name_return].expanding().std()\n",
    "\n",
    "    def create_indicator(self, name_return, factor):\n",
    "        self.dataframe[f'Indicator_{name_return}'] = self.dataframe.apply(lambda row:\n",
    "                                                                          catalog_return(row, factor, name_return),\n",
    "                                                                          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#empresas = [\"ANIM3.SA\", \"AZUL4.SA\", \"GOLL4.SA\", \"BBAS3.SA\", \"MGLU3.SA\", \"VVAR4.SA\", \"ITUB4.SA\", \"BBDC3.SA\", \"SANB11.SA\", \"NTCO3.SA\", \"PETR4.SA\", \"TOTS3.SA\", \"VALE3.SA\"]\n",
    "#empresas = [\"\"]\n",
    "\n",
    "for emp in empresas:\n",
    "  HistoricalData(ticker=emp, startDate=\"2010-01-01\", endDate=\"2024-01-01\", path2save=f\"dataset/prices/{emp}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPx8N49d9RwF",
    "outputId": "aed33f5b-546f-4199-d751-d1b8225a851d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['ENBR3.SA']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['SULA11.SA']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['VIIA3.SA']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Baixar precos de varias empresas\n",
    "#empresas = [\"ITUB4.SA\", \"BBDC4.SA\", \"BBAS3.SA\", \"BPAC11.SA\", \"SANB11.SA\"]\n",
    "#empresas = [\"ANIM3.SA\", \"AZUL4.SA\", \"GOLL4.SA\", \"BBAS3.SA\", \"MGLU3.SA\", \"VVAR4.SA\", \"ITUB4.SA\", \"BBDC3.SA\", \"SANB11.SA\", \"NTCO3.SA\", \"PETR4.SA\", \"TOTS3.SA\", \"VALE3.SA\"]\n",
    "#empresas = [\"\"]\n",
    "empresas = [\"BOVA11.SA\",\n",
    "    'ABEV3.SA', 'ANIM3.SA', 'AZUL4.SA', 'BBAS3.SA', 'BBDC3.SA', 'MGLU3.SA', \n",
    "    'NTCO3.SA', 'PETR4.SA', 'SANB11.SA', 'TOTS3.SA', 'VALE3.SA',\n",
    "    'B3SA3.SA', 'BPAC11.SA', 'BRAP4.SA', 'BRFS3.SA', 'BRKM5.SA', \n",
    "    'CIEL3.SA', 'CMIG4.SA', 'CPLE6.SA', 'CSAN3.SA', 'CSNA3.SA', \n",
    "    'CYRE3.SA', 'ECOR3.SA', 'ELET3.SA', 'ELET6.SA', 'EMBR3.SA', \n",
    "    'ENBR3.SA', 'ENGI11.SA', 'EQTL3.SA', 'GGBR4.SA', 'GOAU4.SA', \n",
    "    'GOLL4.SA', 'HAPV3.SA', 'HYPE3.SA', 'ITSA4.SA', 'ITUB4.SA', \n",
    "    'JBSS3.SA', 'KLBN11.SA', 'LREN3.SA', 'MRFG3.SA', 'MRVE3.SA', \n",
    "    'MULT3.SA', 'PCAR3.SA', 'PETR3.SA', 'PRIO3.SA', 'RADL3.SA', \n",
    "    'RAIL3.SA', 'RENT3.SA', 'SBSP3.SA', 'SLCE3.SA', 'SMTO3.SA', \n",
    "    'SULA11.SA', 'SUZB3.SA', 'TAEE11.SA', 'TIMS3.SA', 'UGPA3.SA', \n",
    "    'USIM5.SA', 'VBBR3.SA', 'VIIA3.SA', 'WEGE3.SA', 'YDUQ3.SA'\n",
    "]\n",
    "\n",
    "for emp in empresas:\n",
    "  HistoricalData(ticker=emp, startDate=\"2010-01-01\", endDate=\"2024-01-01\", path2save=f\"dataset/prices/{emp}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:40:42.560009Z",
     "start_time": "2024-08-29T00:40:42.554366Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTfi5b38_Vq3",
    "outputId": "249ea867-e7bb-4257-d8cb-f1e0876a1785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File BBAS3.SA.csv created and save in dataset/prices_processed/BBAS3.SA.csv\n",
      "File BBDC4.SA.csv created and save in dataset/prices_processed/BBDC4.SA.csv\n",
      "File BPAC11.SA.csv created and save in dataset/prices_processed/BPAC11.SA.csv\n",
      "File ITUB4.SA.csv created and save in dataset/prices_processed/ITUB4.SA.csv\n",
      "File SANB11.SA.csv created and save in dataset/prices_processed/SANB11.SA.csv\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('dataset/prices')\n",
    "for file in files:\n",
    "    data_processed = DataProcessing(pd.read_csv(f'dataset/prices/{file}'))\n",
    "    data_processed.create_return_by_period(name_return='Daily_Return', period=1, remove_nan=False)\n",
    "    data_processed.create_return_by_period(name_return='Week_Return', period=6, remove_nan=False)\n",
    "    data_processed.create_return_by_period(name_return='Month_Return', period=22, remove_nan=False)\n",
    "    data_processed.create_cumulative_std(name_return='Daily_Return')\n",
    "    data_processed.create_cumulative_std(name_return='Week_Return')\n",
    "    data_processed.create_cumulative_std(name_return='Month_Return')\n",
    "    data_processed.create_indicator(name_return='Daily_Return', factor=0.1)\n",
    "    data_processed.create_indicator(name_return='Week_Return', factor=0.1)\n",
    "    data_processed.create_indicator(name_return='Month_Return', factor=0.1)\n",
    "    data_processed.dataframe.to_csv(f'dataset/prices_processed/{file}', index_label=False)\n",
    "    print(f'File {file} created and save in dataset/prices_processed/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:39:01.131455Z",
     "start_time": "2024-08-29T00:39:01.127686Z"
    },
    "id": "HRkz7Zn-Jbk1"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "#userName = os.getenv(\"USERNAME\")\n",
    "#password = os.getenv(\"PASSWORD\")\n",
    "userName =\"aluno.thiago.nunes\" # os.getenv(\"USERNAME\")\n",
    "password = \"NLPfinance2@23\" #os.getenv(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:41:11.307151Z",
     "start_time": "2024-08-29T00:41:11.302292Z"
    },
    "collapsed": true,
    "id": "Q2tpE1IBA9dJ"
   },
   "outputs": [],
   "source": [
    "def EventsDate(ticker, userName=userName, password=password, startDate='01012010', endDate=\"01012024\"):\n",
    "  url = \"https://www.comdinheiro.com.br/Clientes/API/EndPoint001.php\"\n",
    "  querystring = {\"code\":\"import_data\"}\n",
    "  payload = f\"username={userName}&password={password}&URL=HistoricoIndicadoresFundamentalistas001.php%3F%26data_ini%3D{startDate}%26data_fim%3D{endDate}%26trailing%3D12%26conv%3DMIXED%26moeda%3DMOEDA_ORIGINAL%26c_c%3Dconsolidado%26m_m%3D1000000%26n_c%3D5%26f_v%3D1%26papel%3D{ticker}%26indic%3DNOME_EMPRESA%2BRL%2BLL%2BEBITDA%2BDATA_PUBLICACAO%2BPRECO_ABERTURA%2BPRECO_FECHAMENTO%26periodicidade%3Dtri%26graf_tab%3Dtabela%26desloc_data_analise%3D1%26flag_transpor%3D0%26c_d%3Dd%26enviar_email%3D0%26enviar_email_log%3D0%26cabecalho_excel%3Dmodo1%26relat_alias_automatico%3Dcmd_alias_01&format=json3\"\n",
    "  headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "  response = requests.request(\"POST\", url, data=payload, headers=headers, params=querystring)\n",
    "  data = json.loads(response.text)\n",
    "  df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "  novas_colunas = [\"Data\", \"Empresa\", \"Receita\", \"Lucro\", \"EBITDA\", \"Data_Publicacao\", \"Preco_Abertura\", \"Preco_fechamento\", \"Consolidado\", \"Convencao\", \"Moeda\", \"Data_Demonstracao\", \"Meses\", \"Data_Analise\"]\n",
    "  df.columns = novas_colunas\n",
    "  df = df.drop(\"lin0\")\n",
    "  df['Data_Publicacao'] = pd.to_datetime(df['Data_Publicacao'], errors = 'coerce', format='%d/%m/%Y')\n",
    "  df.reset_index(drop=True, inplace=True)\n",
    "  df['Data_Publicacao'] = pd.to_datetime(df['Data_Publicacao'], format='%d/%m/%Y').dt.strftime('%Y-%m-%d')\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:39:47.095505Z",
     "start_time": "2024-08-29T00:39:03.048464Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYrHaVrZCy3G",
    "outputId": "20c8db17-f3a9-4a71-f193-1bee01f80ffd"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('dataset/prices_processed')\n",
    "for emp in files:\n",
    "  price_p = pd.read_csv(f\"dataset/prices_processed/{emp}\")\n",
    "  date_df = EventsDate(ticker= emp[0:-7])\n",
    "  price_p.insert(1, 'event', price_p['Date'].apply(lambda date: 1 if date in date_df['Data_Publicacao'].values else 0))\n",
    "  price_p.to_csv(f\"dataset/prices_processed/{emp}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:25:53.242985Z",
     "start_time": "2024-08-29T00:25:53.224395Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "y4CQ0Anuhz1I",
    "outputId": "2f00dda2-e37a-4feb-f328-5a0572a261de"
   },
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv(\"dataset/prices_processed/ITUB4.SA.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>event</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>WClose</th>\n",
       "      <th>Daily_Return</th>\n",
       "      <th>Week_Return</th>\n",
       "      <th>Month_Return</th>\n",
       "      <th>Cumulative_std_Daily_Return</th>\n",
       "      <th>Cumulative_std_Week_Return</th>\n",
       "      <th>Cumulative_std_Month_Return</th>\n",
       "      <th>Indicator_Daily_Return</th>\n",
       "      <th>Indicator_Week_Return</th>\n",
       "      <th>Indicator_Month_Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>17.708261</td>\n",
       "      <td>18.436810</td>\n",
       "      <td>17.708261</td>\n",
       "      <td>18.268333</td>\n",
       "      <td>10.207858</td>\n",
       "      <td>11843397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>18.313868</td>\n",
       "      <td>18.386723</td>\n",
       "      <td>18.168158</td>\n",
       "      <td>18.386723</td>\n",
       "      <td>10.274011</td>\n",
       "      <td>8593315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0</td>\n",
       "      <td>18.327526</td>\n",
       "      <td>18.436810</td>\n",
       "      <td>18.077089</td>\n",
       "      <td>18.227352</td>\n",
       "      <td>10.184958</td>\n",
       "      <td>10602572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>18.099855</td>\n",
       "      <td>18.236460</td>\n",
       "      <td>18.008787</td>\n",
       "      <td>18.040663</td>\n",
       "      <td>10.080639</td>\n",
       "      <td>9966567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>18.113516</td>\n",
       "      <td>18.113516</td>\n",
       "      <td>17.721922</td>\n",
       "      <td>17.767456</td>\n",
       "      <td>9.927980</td>\n",
       "      <td>9748709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>2023-12-21</td>\n",
       "      <td>0</td>\n",
       "      <td>32.750000</td>\n",
       "      <td>32.869999</td>\n",
       "      <td>32.549999</td>\n",
       "      <td>32.790001</td>\n",
       "      <td>31.130529</td>\n",
       "      <td>21813000</td>\n",
       "      <td>34.060576</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.074295</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.045765</td>\n",
       "      <td>0.088128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>0</td>\n",
       "      <td>32.820000</td>\n",
       "      <td>33.540001</td>\n",
       "      <td>32.799999</td>\n",
       "      <td>33.310001</td>\n",
       "      <td>31.624207</td>\n",
       "      <td>32650500</td>\n",
       "      <td>35.998260</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.013297</td>\n",
       "      <td>0.083859</td>\n",
       "      <td>0.019699</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>0.088125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>0</td>\n",
       "      <td>33.369999</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>33.230000</td>\n",
       "      <td>33.650002</td>\n",
       "      <td>31.947004</td>\n",
       "      <td>23135400</td>\n",
       "      <td>36.629509</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>0.019697</td>\n",
       "      <td>0.045754</td>\n",
       "      <td>0.088125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>0</td>\n",
       "      <td>33.610001</td>\n",
       "      <td>33.950001</td>\n",
       "      <td>33.570000</td>\n",
       "      <td>33.880001</td>\n",
       "      <td>32.165359</td>\n",
       "      <td>18176500</td>\n",
       "      <td>37.201332</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.029656</td>\n",
       "      <td>0.107648</td>\n",
       "      <td>0.019694</td>\n",
       "      <td>0.045750</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>0</td>\n",
       "      <td>33.880001</td>\n",
       "      <td>33.970001</td>\n",
       "      <td>33.740002</td>\n",
       "      <td>33.970001</td>\n",
       "      <td>32.250809</td>\n",
       "      <td>12555300</td>\n",
       "      <td>32.840667</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.031701</td>\n",
       "      <td>0.112912</td>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.045746</td>\n",
       "      <td>0.088134</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3473 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  event       Open       High        Low      Close  \\\n",
       "0     2010-01-04      0  17.708261  18.436810  17.708261  18.268333   \n",
       "1     2010-01-05      0  18.313868  18.386723  18.168158  18.386723   \n",
       "2     2010-01-06      0  18.327526  18.436810  18.077089  18.227352   \n",
       "3     2010-01-07      0  18.099855  18.236460  18.008787  18.040663   \n",
       "4     2010-01-08      0  18.113516  18.113516  17.721922  17.767456   \n",
       "...          ...    ...        ...        ...        ...        ...   \n",
       "3468  2023-12-21      0  32.750000  32.869999  32.549999  32.790001   \n",
       "3469  2023-12-22      0  32.820000  33.540001  32.799999  33.310001   \n",
       "3470  2023-12-26      0  33.369999  33.779999  33.230000  33.650002   \n",
       "3471  2023-12-27      0  33.610001  33.950001  33.570000  33.880001   \n",
       "3472  2023-12-28      0  33.880001  33.970001  33.740002  33.970001   \n",
       "\n",
       "      Adj Close    Volume     WClose  Daily_Return  Week_Return  Month_Return  \\\n",
       "0     10.207858  11843397        NaN           NaN          NaN           NaN   \n",
       "1     10.274011   8593315        NaN      0.006460          NaN           NaN   \n",
       "2     10.184958  10602572        NaN     -0.008706          NaN           NaN   \n",
       "3     10.080639   9966567        NaN     -0.010295          NaN           NaN   \n",
       "4      9.927980   9748709        NaN     -0.015260          NaN           NaN   \n",
       "...         ...       ...        ...           ...          ...           ...   \n",
       "3468  31.130529  21813000  34.060576      0.006732     0.005812      0.074295   \n",
       "3469  31.624207  32650500  35.998260      0.015734     0.013297      0.083859   \n",
       "3470  31.947004  23135400  36.629509      0.010156     0.027721      0.095310   \n",
       "3471  32.165359  18176500  37.201332      0.006812     0.029656      0.107648   \n",
       "3472  32.250809  12555300  32.840667      0.002653     0.031701      0.112912   \n",
       "\n",
       "      Cumulative_std_Daily_Return  Cumulative_std_Week_Return  \\\n",
       "0                             NaN                         NaN   \n",
       "1                             NaN                         NaN   \n",
       "2                        0.010723                         NaN   \n",
       "3                        0.009249                         NaN   \n",
       "4                        0.009366                         NaN   \n",
       "...                           ...                         ...   \n",
       "3468                     0.019700                    0.045765   \n",
       "3469                     0.019699                    0.045759   \n",
       "3470                     0.019697                    0.045754   \n",
       "3471                     0.019694                    0.045750   \n",
       "3472                     0.019691                    0.045746   \n",
       "\n",
       "      Cumulative_std_Month_Return  Indicator_Daily_Return  \\\n",
       "0                             NaN                       0   \n",
       "1                             NaN                       0   \n",
       "2                             NaN                      -1   \n",
       "3                             NaN                      -1   \n",
       "4                             NaN                      -1   \n",
       "...                           ...                     ...   \n",
       "3468                     0.088128                       1   \n",
       "3469                     0.088125                       1   \n",
       "3470                     0.088125                       1   \n",
       "3471                     0.088129                       1   \n",
       "3472                     0.088134                       1   \n",
       "\n",
       "      Indicator_Week_Return  Indicator_Month_Return  \n",
       "0                         0                       0  \n",
       "1                         0                       0  \n",
       "2                         0                       0  \n",
       "3                         0                       0  \n",
       "4                         0                       0  \n",
       "...                     ...                     ...  \n",
       "3468                      1                       1  \n",
       "3469                      1                       1  \n",
       "3470                      1                       1  \n",
       "3471                      1                       1  \n",
       "3472                      1                       1  \n",
       "\n",
       "[3473 rows x 18 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNnVI9scDHKe"
   },
   "outputs": [],
   "source": [
    "def separate_returns(final_data):\n",
    "    # Inicializar colunas se não existirem\n",
    "    if 'return_daily' not in final_data.columns:\n",
    "        final_data['return_daily'] = np.nan\n",
    "    if 'return_week' not in final_data.columns:\n",
    "        final_data['return_week'] = np.nan\n",
    "    if 'return_month' not in final_data.columns:\n",
    "        final_data['return_month'] = np.nan\n",
    "\n",
    "    first_return_daily_list = []\n",
    "    remaining_return_daily_list = []\n",
    "    first_return_week_list = []\n",
    "    remaining_return_week_list = []\n",
    "    first_return_month_list = []\n",
    "    remaining_return_month_list = []\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    while start_idx < len(final_data):\n",
    "        # Encontra o evento\n",
    "        if 1 in final_data[start_idx:]['event'].values:\n",
    "            event_idx = final_data[start_idx:]['event'].eq(1).idxmax()\n",
    "        else:\n",
    "            break\n",
    "        # Encontrar o próximo evento\n",
    "        if 1 in final_data[event_idx+1:]['event'].values:\n",
    "            prox_event_idx = final_data[event_idx+1:]['event'].eq(1).idxmax()\n",
    "        else:\n",
    "            prox_event_idx = len(final_data)\n",
    "\n",
    "        # Calcular o primeiro retorno diario logo após o evento\n",
    "        if event_idx + 1 < len(final_data):\n",
    "            final_data.loc[event_idx + 1, 'return_daily'] = np.log(final_data.loc[event_idx + 1, 'Close'] / final_data.loc[event_idx, 'Close'])\n",
    "            first_return_daily_list.append(final_data.iloc[event_idx + 1])\n",
    "\n",
    "        # Calcular os retornos diarios restantes até o próximo evento\n",
    "        for i in range(event_idx + 2, prox_event_idx, 1):\n",
    "            final_data.loc[i, 'return_daily'] = np.log(final_data.loc[i, 'Close'] / final_data.loc[i-1, 'Close'])\n",
    "            remaining_return_daily_list.append(final_data.iloc[i])\n",
    "\n",
    "        # Calcular o primeiro retorno semanal logo após o evento\n",
    "        if event_idx + 5 < len(final_data):\n",
    "            final_data.loc[event_idx + 5, 'return_week'] = np.log(final_data.loc[event_idx + 5, 'Close'] / final_data.loc[event_idx, 'Close'])\n",
    "            first_return_week_list.append(final_data.iloc[event_idx + 5])\n",
    "\n",
    "        # Calcular os retornos semanais restantes até o próximo evento\n",
    "        for i in range(event_idx + 10, prox_event_idx, 5):\n",
    "            final_data.loc[i, 'return_week'] = np.log(final_data.loc[i, 'Close'] / final_data.loc[i-5, 'Close'])\n",
    "            remaining_return_week_list.append(final_data.iloc[i])\n",
    "\n",
    "\n",
    "        # Calcular o primeiro retorno mensal logo após o evento\n",
    "        if event_idx + 21 < len(final_data):\n",
    "            final_data.loc[event_idx + 21, 'return_month'] = np.log(final_data.loc[event_idx + 21, 'Close'] / final_data.loc[event_idx, 'Close'])\n",
    "            first_return_month_list.append(final_data.iloc[event_idx + 21])\n",
    "\n",
    "        # Calcular os retornos mensais restantes até o próximo evento\n",
    "        for i in range(event_idx + 42, prox_event_idx, 22):\n",
    "            final_data.loc[i, 'return_month'] = np.log(final_data.loc[i, 'Close'] / final_data.loc[i-21, 'Close'])\n",
    "            remaining_return_month_list.append(final_data.iloc[i])\n",
    "\n",
    "        # Reinicia após o evento\n",
    "        start_idx = event_idx + 1\n",
    "\n",
    "    # Criar DataFrames para o primeiro e os demais retornos semanais e mensais\n",
    "    first_return_daily_df = pd.DataFrame(first_return_daily_list).reset_index()[['index',\n",
    "    'Close', 'Date'\n",
    ", 'event', 'return_daily']]\n",
    "    remaining_return_daily_df = pd.DataFrame(remaining_return_daily_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_daily']]\n",
    "    first_return_week_df = pd.DataFrame(first_return_week_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_week']]\n",
    "    remaining_return_week_df = pd.DataFrame(remaining_return_week_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_week']]\n",
    "    first_return_month_df = pd.DataFrame(first_return_month_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_month']]\n",
    "    remaining_return_month_df = pd.DataFrame(remaining_return_month_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_month']]\n",
    "\n",
    "    first_return_daily_df.rename(columns={'return_daily': 'return'}, inplace=True)\n",
    "    remaining_return_daily_df.rename(columns={'return_daily': 'return'}, inplace=True)\n",
    "    first_return_week_df.rename(columns={'return_week': 'return'}, inplace=True)\n",
    "    remaining_return_week_df.rename(columns={'return_week': 'return'}, inplace=True)\n",
    "    first_return_month_df.rename(columns={'return_month': 'return'}, inplace=True)\n",
    "    remaining_return_month_df.rename(columns={'return_month': 'return'}, inplace=True)\n",
    "\n",
    "    return first_return_daily_df, remaining_return_daily_df, first_return_week_df, remaining_return_week_df, first_return_month_df, remaining_return_month_df\n",
    "\n",
    "\n",
    "first_return_daily_df, remaining_return_daily_df, first_return_week_df, remaining_return_week_df, first_return_month_df, remaining_return_month_df = separate_returns(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp8C0lHqjyQb"
   },
   "source": [
    "# Análise de Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSuAMo05moiU"
   },
   "outputs": [],
   "source": [
    "name_prices = {\n",
    "    'azul': 'AZUL4.SA',\n",
    "    'BB': 'BBAS3.SA',\n",
    "    'bradesco': 'BBDC4.SA',\n",
    "    'brf': 'BRFS3.SA',\n",
    "    'ccr': 'CCRO3.SA',\n",
    "    'cosan': 'CSAN3.SA',\n",
    "    'cpfl_energia': 'CPFE3.SA',\n",
    "    'dasa': 'DASA3.SA',\n",
    "    'fleury': 'FLRY3.SA',\n",
    "    'gol': 'GOLL4.SA',\n",
    "    'hapvida': 'HAPV3.SA',\n",
    "    'itau': 'ITUB4.SA',\n",
    "    'locaweb': 'LWSA3.SA',\n",
    "    'magazine_luiza': 'MGLU3.SA',\n",
    "    'mrv_engenharia': 'MRVE3.SA',\n",
    "    'natura': 'NTCO3.SA',\n",
    "    'petrobras': 'PETR4.SA',\n",
    "    'santander': 'SANB11.SA',\n",
    "    'sul_america': 'SULA11.SA',\n",
    "    'totvs': 'TOTS3.SA',\n",
    "    'vale': 'VALE3.SA',\n",
    "    'via_varejo': 'BHIA3.SA'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def contar_indicadores(dataframe1, dataframe2, indicador, tipo=1):\n",
    "    contagem = []\n",
    "    for i, row in dataframe1.iterrows():\n",
    "        inicio = row['data']\n",
    "        fim = row['prox_reuniao']\n",
    "        if not pd.isna(fim):\n",
    "            filtro = dataframe2[(dataframe2['Date'] >= inicio) & (dataframe2['Date'] <= fim) & (dataframe2[f'Indicator_{indicador}'] == tipo)]\n",
    "        else:\n",
    "            fim = inicio + pd.DateOffset(months=3)\n",
    "            filtro = dataframe2[(dataframe2['Date'] >= inicio) & (dataframe2['Date'] <= fim) & (dataframe2[f'Indicator_{indicador}'] == tipo)]\n",
    "        contagem.append(len(filtro))\n",
    "    return contagem\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Abrir o arquivo PDF\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        # Criar um objeto PDF Reader\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Inicializar uma variável para armazenar o texto\n",
    "        all_text = \"\"\n",
    "\n",
    "        # Iterar sobre cada página e extrair o texto\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            all_text += page.extract_text()\n",
    "    # Remover os caracteres de nova linha\n",
    "    all_text = all_text.replace('\\n', ' ')\n",
    "    # Segmentar o texto usando \".\" como separador\n",
    "    segments = all_text.split('. ')\n",
    "\n",
    "    # Remover espaços em branco desnecessários e segmentos vazios\n",
    "    segments = [segment.strip() for segment in segments if segment.strip() and len(segment.strip()) >= 15]\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yQ6z_TimxCm",
    "outputId": "03c1656a-2e7e-4861-cc15-a8a56a2b5147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'magazine_luiza']\n",
      "Skipping file: .ipynb_checkpoints\n",
      "Transcript and returns file already exists for: MGLU3.SA\n"
     ]
    }
   ],
   "source": [
    "file_data_transcripts = os.listdir('dataset/transcripts')\n",
    "print(file_data_transcripts)\n",
    "for file in file_data_transcripts:\n",
    "    if file not in name_prices:\n",
    "        print(f\"Skipping file: {file}\")\n",
    "        continue\n",
    "    if os.path.exists(f'dataset/transcripts_and_returns/{name_prices[file]}.csv'):\n",
    "        print(\"Transcript and returns file already exists for: \" + name_prices[file])\n",
    "        continue\n",
    "    if not os.path.exists(f'dataset/transcripts/{file}/datas.csv'):\n",
    "        print(\"Data file does not exist: \"+file)\n",
    "        continue\n",
    "    df = pd.read_csv(f'dataset/transcripts/{file}/datas.csv')\n",
    "    df = df.sort_values(by=['data'], ignore_index=True)\n",
    "    df['data'] = pd.to_datetime(df['data'])\n",
    "    df['prox_reuniao'] = df['data'].shift(-1)\n",
    "    df['prox_reuniao'] = pd.to_datetime(df['prox_reuniao'])\n",
    "    prices = pd.read_csv(f'dataset/prices_processed/{name_prices[file]}.csv')\n",
    "    prices['Date'] = pd.to_datetime(prices['Date'])\n",
    "    df['Daily_Return_Positive'] = contar_indicadores(df, prices, 'Daily_Return', tipo=1)\n",
    "    df['Daily_Return_Neutral'] = contar_indicadores(df, prices, 'Daily_Return', tipo=0)\n",
    "    df['Daily_Return_Negative'] = contar_indicadores(df, prices, 'Daily_Return', tipo=-1)\n",
    "    df['Daily_Return_Total'] = df['Daily_Return_Positive'] + df['Daily_Return_Neutral'] + df['Daily_Return_Negative']\n",
    "    df['Week_Return_Positive'] = contar_indicadores(df, prices, 'Week_Return', tipo=1)\n",
    "    df['Week_Return_Neutral'] = contar_indicadores(df, prices, 'Week_Return', tipo=0)\n",
    "    df['Week_Return_Negative'] = contar_indicadores(df, prices, 'Week_Return', tipo=-1)\n",
    "    df['Week_Return_Total'] = df['Week_Return_Positive'] + df['Week_Return_Neutral'] + df['Week_Return_Negative']\n",
    "    df['Month_Return_Positive'] = contar_indicadores(df, prices, 'Month_Return', tipo=1)\n",
    "    df['Month_Return_Neutral'] = contar_indicadores(df, prices, 'Month_Return', tipo=0)\n",
    "    df['Month_Return_Negative'] = contar_indicadores(df, prices, 'Month_Return', tipo=-1)\n",
    "    df['Month_Return_Total'] = df['Month_Return_Positive'] + df['Month_Return_Neutral'] + df['Month_Return_Negative']\n",
    "    df.to_csv(f'dataset/transcripts_and_returns/{name_prices[file]}.csv', index=False)\n",
    "    print(f\"Transcript and return created in: {name_prices[file]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xGPZnxOjtZY",
    "outputId": "0aba62b4-db50-42cd-d65e-d924e830a81b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred_mapper = {\n",
    "    0: 1,\n",
    "    1: -1,\n",
    "    2: 0\n",
    "  }\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "finbertptbr = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGXlCKUJjw9L",
    "outputId": "0dd3f26d-8565-4130-e827-8b019ae4762d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocorreu um erro: name 'pd' is not defined\n"
     ]
    }
   ],
   "source": [
    "files = ['magazine_luiza']  # os.listdir(\"../../dataset/transcripts\")\n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_csv(f\"dataset/transcripts_and_returns/{name_prices[file]}.csv\")\n",
    "        for i, row in df.iterrows():\n",
    "            # if not pd.isna(row['positive_sentiment']):\n",
    "            #     print(row['positive_sentiment'])\n",
    "            #     continue\n",
    "            text_extract_name = row['trimestre']\n",
    "            text = extract_text_from_pdf(f'dataset/transcripts/{file}/{text_extract_name}.pdf')\n",
    "            tokens = tokenizer(text, return_tensors=\"pt\",\n",
    "                                   padding=True, truncation=True, max_length=512)\n",
    "\n",
    "            finbertptbr_outputs = finbertptbr(**tokens)\n",
    "            preds = [pred_mapper[np.argmax(pred)] for pred in finbertptbr_outputs.logits.cpu().detach().numpy()]\n",
    "            posi, neut, nega = preds.count(1), preds.count(0), preds.count(-1)\n",
    "            df.loc[i, 'positive_sentiment'] = posi\n",
    "            df.loc[i, 'neutral_sentiment'] = neut\n",
    "            df.loc[i, 'negative_sentiment'] = nega\n",
    "            df.to_csv(f\"dataset/transcripts_and_returns/{name_prices[file]}.csv\", index=False)\n",
    "\n",
    "            del tokens, finbertptbr_outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro de arquivo não encontrado: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwYIZBn8Tw0u"
   },
   "source": [
    "# Previsão de Preços"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "87IYGmTRUALm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcOJSna7Twf5",
    "outputId": "127b21b9-6431-4307-c994-e602783ce8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro Quadrático Médio: 0.8151249761458873\n",
      "Preço previsto para 2024-08-11: 17.2836993598938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Criar um DataFrame\n",
    "df = pd.read_csv('dataset/prices/AZUL4.SA.csv')\n",
    "\n",
    "# Converter a coluna 'Date' para o formato datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extrair recursos da data (Ano, Mês, Dia) para usar como variáveis independentes\n",
    "df['Ano'] = df['Date'].dt.year\n",
    "df['Mes'] = df['Date'].dt.month\n",
    "df['Dia'] = df['Date'].dt.day\n",
    "\n",
    "# Definir variáveis independentes (X) e dependentes (y)\n",
    "X = df[['Ano', 'Mes', 'Dia']]  # variáveis independentes\n",
    "y = df['Close']  # variável dependente\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar o modelo Random Forest\n",
    "modelo = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo usando o erro quadrático médio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Erro Quadrático Médio: {mse}')\n",
    "\n",
    "# Prever o preço para uma nova data (ex: 2024-08-11)\n",
    "nova_data = pd.to_datetime('2024-08-11')\n",
    "ano = nova_data.year\n",
    "mes = nova_data.month\n",
    "dia = nova_data.day\n",
    "\n",
    "novo_dia = np.array([[ano, mes, dia]])\n",
    "preco_previsto = modelo.predict(novo_dia)\n",
    "print(f'Preço previsto para 2024-08-11: {preco_previsto[0]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
