{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:19:33.808461Z",
     "start_time": "2024-08-29T00:19:27.070412Z"
    },
    "id": "uep96yWDqBq1"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import PyPDF2\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLenLtoyj7d_"
   },
   "source": [
    "# Coleta de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:19:34.409839Z",
     "start_time": "2024-08-29T00:19:34.405412Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1So0ez1UiLja",
    "outputId": "ceed1e57-1d01-4308-8474-97c924df2ce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta 'dataset' foi criada.\n",
      "Pasta 'dataset/prices' foi criada.\n",
      "Pasta 'dataset/prices_processed' foi criada.\n"
     ]
    }
   ],
   "source": [
    "folders = [\"dataset\", \"dataset/prices\", \"dataset/prices_processed\"]\n",
    "\n",
    "# Verifica se as pastas existem, se não, cria-as\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Pasta '{folder}' foi criada.\")\n",
    "    else:\n",
    "        print(f\"Pasta '{folder}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "eHjgdyxtqEgX",
    "outputId": "39f959b0-8b14-4b7d-b9a1-1b352933d58a"
   },
   "outputs": [],
   "source": [
    "# API YahooFinance para baixar os dados historicos\n",
    "def HistoricalData(ticker, startDate, endDate, path2save = ''):\n",
    "\n",
    "  \"\"\"\n",
    "  ticker: Simbolo ação. Ex: VALE\n",
    "  startDate: Data inicial. Ex: 2010-01-01\n",
    "  endDate: Data final. Ex: 2020-12-31\n",
    "  path2save: Caminho para salvar o dataframe.\n",
    "  \"\"\"\n",
    "  data = yf.download(ticker, start=startDate, end=endDate)\n",
    "  df = pd.DataFrame(data)\n",
    "\n",
    "  if path2save != '':\n",
    "    df.to_csv(path2save)\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "#dados = HistoricalData(\"ANIM3.SA\", startDate='2010-01-01', endDate='2023-01-01')\n",
    "#dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:19:39.303697Z",
     "start_time": "2024-08-29T00:19:39.295354Z"
    },
    "id": "_97em83mqMFQ"
   },
   "outputs": [],
   "source": [
    "# Criando ferramenta para adicionar novos campos para o dataframe\n",
    "def catalog_return(row, x, name_return):\n",
    "    if row[name_return] > x * row[f'Cumulative_std_{name_return}']:\n",
    "        return 1\n",
    "    elif row[name_return] < -x * row[f'Cumulative_std_{name_return}']:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "class DataProcessing:\n",
    "    def __init__(self, data):\n",
    "        self.dataframe = data\n",
    "        self.dataframe['Date'] = pd.to_datetime(self.dataframe['Date'])\n",
    "        self.dataframe = self.dataframe.sort_values(by='Date')\n",
    "\n",
    "    def get_by_date_range(self, start_date, end_date):\n",
    "        mask = ((self.dataframe['Date'] >= start_date) & (self.dataframe['Date'] <= end_date))\n",
    "        return self.dataframe.loc[mask]\n",
    "\n",
    "    def get_by_date(self, date):\n",
    "        return self.dataframe.loc[(self.dataframe['Date'] == date)]\n",
    "\n",
    "    def create_return_by_period(self, name_return, period, remove_nan=False):\n",
    "        self.dataframe[f'{name_return}'] = np.log(\n",
    "            self.dataframe['Close'] / self.dataframe['Close'].shift(period))\n",
    "        if remove_nan:\n",
    "            self.dataframe = self.dataframe.dropna()\n",
    "\n",
    "    def create_cumulative_std(self, name_return):\n",
    "        self.dataframe[f'Cumulative_std_{name_return}'] = self.dataframe[name_return].expanding().std()\n",
    "\n",
    "    def create_indicator(self, name_return, factor):\n",
    "        self.dataframe[f'Indicator_{name_return}'] = self.dataframe.apply(lambda row:\n",
    "                                                                          catalog_return(row, factor, name_return),\n",
    "                                                                          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPx8N49d9RwF",
    "outputId": "aed33f5b-546f-4199-d751-d1b8225a851d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['ENBR3.SA']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['SULA11.SA']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['VIIA3.SA']: YFTzMissingError('$%ticker%: possibly delisted; No timezone found')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Baixar precos de varias empresas\n",
    "#empresas = [\"ITUB4.SA\", \"BBDC4.SA\", \"BBAS3.SA\", \"BPAC11.SA\", \"SANB11.SA\"]\n",
    "#empresas = [\"ANIM3.SA\", \"AZUL4.SA\", \"GOLL4.SA\", \"BBAS3.SA\", \"MGLU3.SA\", \"VVAR4.SA\", \"ITUB4.SA\", \"BBDC3.SA\", \"SANB11.SA\", \"NTCO3.SA\", \"PETR4.SA\", \"TOTS3.SA\", \"VALE3.SA\"]\n",
    "#empresas = [\"\"]\n",
    "empresas = [\"BOVA11.SA\",\n",
    "    'ABEV3.SA', 'ANIM3.SA', 'AZUL4.SA', 'BBAS3.SA', 'BBDC3.SA', 'MGLU3.SA', \n",
    "    'NTCO3.SA', 'PETR4.SA', 'SANB11.SA', 'TOTS3.SA', 'VALE3.SA',\n",
    "    'B3SA3.SA', 'BPAC11.SA', 'BRAP4.SA', 'BRFS3.SA', 'BRKM5.SA', \n",
    "    'CIEL3.SA', 'CMIG4.SA', 'CPLE6.SA', 'CSAN3.SA', 'CSNA3.SA', \n",
    "    'CYRE3.SA', 'ECOR3.SA', 'ELET3.SA', 'ELET6.SA', 'EMBR3.SA', \n",
    "    'ENBR3.SA', 'ENGI11.SA', 'EQTL3.SA', 'GGBR4.SA', 'GOAU4.SA', \n",
    "    'GOLL4.SA', 'HAPV3.SA', 'HYPE3.SA', 'ITSA4.SA', 'ITUB4.SA', \n",
    "    'JBSS3.SA', 'KLBN11.SA', 'LREN3.SA', 'MRFG3.SA', 'MRVE3.SA', \n",
    "    'MULT3.SA', 'PCAR3.SA', 'PETR3.SA', 'PRIO3.SA', 'RADL3.SA', \n",
    "    'RAIL3.SA', 'RENT3.SA', 'SBSP3.SA', 'SLCE3.SA', 'SMTO3.SA', \n",
    "    'SULA11.SA', 'SUZB3.SA', 'TAEE11.SA', 'TIMS3.SA', 'UGPA3.SA', \n",
    "    'USIM5.SA', 'VBBR3.SA', 'VIIA3.SA', 'WEGE3.SA', 'YDUQ3.SA'\n",
    "]\n",
    "\n",
    "for emp in empresas:\n",
    "  HistoricalData(ticker=emp, startDate=\"2010-01-01\", endDate=\"2024-01-01\", path2save=f\"dataset/prices/{emp}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:40:42.560009Z",
     "start_time": "2024-08-29T00:40:42.554366Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTfi5b38_Vq3",
    "outputId": "249ea867-e7bb-4257-d8cb-f1e0876a1785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ABEV3.SA.csv created and save in dataset/prices_processed/ABEV3.SA.csv\n",
      "File ANIM3.SA.csv created and save in dataset/prices_processed/ANIM3.SA.csv\n",
      "File AZUL4.SA.csv created and save in dataset/prices_processed/AZUL4.SA.csv\n",
      "File B3SA3.SA.csv created and save in dataset/prices_processed/B3SA3.SA.csv\n",
      "File BBAS3.csv created and save in dataset/prices_processed/BBAS3.csv\n",
      "File BBAS3.SA.csv created and save in dataset/prices_processed/BBAS3.SA.csv\n",
      "File BBDC3.SA.csv created and save in dataset/prices_processed/BBDC3.SA.csv\n",
      "File BBDC4.csv created and save in dataset/prices_processed/BBDC4.csv\n",
      "File BBDC4.SA.csv created and save in dataset/prices_processed/BBDC4.SA.csv\n",
      "File BOVA11.SA.csv created and save in dataset/prices_processed/BOVA11.SA.csv\n",
      "File BPAC11.csv created and save in dataset/prices_processed/BPAC11.csv\n",
      "File BPAC11.SA.csv created and save in dataset/prices_processed/BPAC11.SA.csv\n",
      "File BRAP4.SA.csv created and save in dataset/prices_processed/BRAP4.SA.csv\n",
      "File BRFS3.SA.csv created and save in dataset/prices_processed/BRFS3.SA.csv\n",
      "File BRKM5.SA.csv created and save in dataset/prices_processed/BRKM5.SA.csv\n",
      "File CIEL3.SA.csv created and save in dataset/prices_processed/CIEL3.SA.csv\n",
      "File CMIG4.SA.csv created and save in dataset/prices_processed/CMIG4.SA.csv\n",
      "File CPLE6.SA.csv created and save in dataset/prices_processed/CPLE6.SA.csv\n",
      "File CSAN3.SA.csv created and save in dataset/prices_processed/CSAN3.SA.csv\n",
      "File CSNA3.SA.csv created and save in dataset/prices_processed/CSNA3.SA.csv\n",
      "File CYRE3.SA.csv created and save in dataset/prices_processed/CYRE3.SA.csv\n",
      "File ECOR3.SA.csv created and save in dataset/prices_processed/ECOR3.SA.csv\n",
      "File ELET3.SA.csv created and save in dataset/prices_processed/ELET3.SA.csv\n",
      "File ELET6.SA.csv created and save in dataset/prices_processed/ELET6.SA.csv\n",
      "File EMBR3.SA.csv created and save in dataset/prices_processed/EMBR3.SA.csv\n",
      "File ENBR3.SA.csv created and save in dataset/prices_processed/ENBR3.SA.csv\n",
      "File ENGI11.SA.csv created and save in dataset/prices_processed/ENGI11.SA.csv\n",
      "File EQTL3.SA.csv created and save in dataset/prices_processed/EQTL3.SA.csv\n",
      "File GGBR4.SA.csv created and save in dataset/prices_processed/GGBR4.SA.csv\n",
      "File GOAU4.SA.csv created and save in dataset/prices_processed/GOAU4.SA.csv\n",
      "File GOLL4.SA.csv created and save in dataset/prices_processed/GOLL4.SA.csv\n",
      "File HAPV3.SA.csv created and save in dataset/prices_processed/HAPV3.SA.csv\n",
      "File HYPE3.SA.csv created and save in dataset/prices_processed/HYPE3.SA.csv\n",
      "File IBOV.csv created and save in dataset/prices_processed/IBOV.csv\n",
      "File IBOV.SA.csv created and save in dataset/prices_processed/IBOV.SA.csv\n",
      "File ITSA4.SA.csv created and save in dataset/prices_processed/ITSA4.SA.csv\n",
      "File ITUB4.csv created and save in dataset/prices_processed/ITUB4.csv\n",
      "File ITUB4.SA.csv created and save in dataset/prices_processed/ITUB4.SA.csv\n",
      "File JBSS3.SA.csv created and save in dataset/prices_processed/JBSS3.SA.csv\n",
      "File KLBN11.SA.csv created and save in dataset/prices_processed/KLBN11.SA.csv\n",
      "File LREN3.SA.csv created and save in dataset/prices_processed/LREN3.SA.csv\n",
      "File MGLU3.SA.csv created and save in dataset/prices_processed/MGLU3.SA.csv\n",
      "File MRFG3.SA.csv created and save in dataset/prices_processed/MRFG3.SA.csv\n",
      "File MRVE3.SA.csv created and save in dataset/prices_processed/MRVE3.SA.csv\n",
      "File MULT3.SA.csv created and save in dataset/prices_processed/MULT3.SA.csv\n",
      "File NTCO3.SA.csv created and save in dataset/prices_processed/NTCO3.SA.csv\n",
      "File PCAR3.SA.csv created and save in dataset/prices_processed/PCAR3.SA.csv\n",
      "File PETR3.SA.csv created and save in dataset/prices_processed/PETR3.SA.csv\n",
      "File PETR4.SA.csv created and save in dataset/prices_processed/PETR4.SA.csv\n",
      "File PRIO3.SA.csv created and save in dataset/prices_processed/PRIO3.SA.csv\n",
      "File RADL3.SA.csv created and save in dataset/prices_processed/RADL3.SA.csv\n",
      "File RAIL3.SA.csv created and save in dataset/prices_processed/RAIL3.SA.csv\n",
      "File RENT3.SA.csv created and save in dataset/prices_processed/RENT3.SA.csv\n",
      "File SANB11.csv created and save in dataset/prices_processed/SANB11.csv\n",
      "File SANB11.SA.csv created and save in dataset/prices_processed/SANB11.SA.csv\n",
      "File SBSP3.SA.csv created and save in dataset/prices_processed/SBSP3.SA.csv\n",
      "File SLCE3.SA.csv created and save in dataset/prices_processed/SLCE3.SA.csv\n",
      "File SMTO3.SA.csv created and save in dataset/prices_processed/SMTO3.SA.csv\n",
      "File SULA11.SA.csv created and save in dataset/prices_processed/SULA11.SA.csv\n",
      "File SUZB3.SA.csv created and save in dataset/prices_processed/SUZB3.SA.csv\n",
      "File TAEE11.SA.csv created and save in dataset/prices_processed/TAEE11.SA.csv\n",
      "File TIMS3.SA.csv created and save in dataset/prices_processed/TIMS3.SA.csv\n",
      "File TOTS3.SA.csv created and save in dataset/prices_processed/TOTS3.SA.csv\n",
      "File UGPA3.SA.csv created and save in dataset/prices_processed/UGPA3.SA.csv\n",
      "File USIM5.SA.csv created and save in dataset/prices_processed/USIM5.SA.csv\n",
      "File VALE3.SA.csv created and save in dataset/prices_processed/VALE3.SA.csv\n",
      "File VBBR3.SA.csv created and save in dataset/prices_processed/VBBR3.SA.csv\n",
      "File VIIA3.SA.csv created and save in dataset/prices_processed/VIIA3.SA.csv\n",
      "File VVAR4.SA.csv created and save in dataset/prices_processed/VVAR4.SA.csv\n",
      "File WEGE3.SA.csv created and save in dataset/prices_processed/WEGE3.SA.csv\n",
      "File YDUQ3.SA.csv created and save in dataset/prices_processed/YDUQ3.SA.csv\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('dataset/prices')\n",
    "for file in files:\n",
    "    data_processed = DataProcessing(pd.read_csv(f'dataset/prices/{file}'))\n",
    "    data_processed.create_return_by_period(name_return='Daily_Return', period=1, remove_nan=False)\n",
    "    data_processed.create_return_by_period(name_return='Week_Return', period=6, remove_nan=False)\n",
    "    data_processed.create_return_by_period(name_return='Month_Return', period=22, remove_nan=False)\n",
    "    data_processed.create_cumulative_std(name_return='Daily_Return')\n",
    "    data_processed.create_cumulative_std(name_return='Week_Return')\n",
    "    data_processed.create_cumulative_std(name_return='Month_Return')\n",
    "    data_processed.create_indicator(name_return='Daily_Return', factor=0.1)\n",
    "    data_processed.create_indicator(name_return='Week_Return', factor=0.1)\n",
    "    data_processed.create_indicator(name_return='Month_Return', factor=0.1)\n",
    "    data_processed.dataframe.to_csv(f'dataset/prices_processed/{file}', index_label=False)\n",
    "    print(f'File {file} created and save in dataset/prices_processed/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:39:01.131455Z",
     "start_time": "2024-08-29T00:39:01.127686Z"
    },
    "id": "HRkz7Zn-Jbk1"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "#userName = os.getenv(\"USERNAME\")\n",
    "#password = os.getenv(\"PASSWORD\")\n",
    "userName =\"aluno.thiago.nunes\" # os.getenv(\"USERNAME\")\n",
    "password = \"NLPfinance2@23\" #os.getenv(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:41:11.307151Z",
     "start_time": "2024-08-29T00:41:11.302292Z"
    },
    "collapsed": true,
    "id": "Q2tpE1IBA9dJ"
   },
   "outputs": [],
   "source": [
    "def EventsDate(ticker, userName=userName, password=password, startDate='01012010', endDate=\"01012024\"):\n",
    "  url = \"https://www.comdinheiro.com.br/Clientes/API/EndPoint001.php\"\n",
    "  querystring = {\"code\":\"import_data\"}\n",
    "  payload = f\"username={userName}&password={password}&URL=HistoricoIndicadoresFundamentalistas001.php%3F%26data_ini%3D{startDate}%26data_fim%3D{endDate}%26trailing%3D12%26conv%3DMIXED%26moeda%3DMOEDA_ORIGINAL%26c_c%3Dconsolidado%26m_m%3D1000000%26n_c%3D5%26f_v%3D1%26papel%3D{ticker}%26indic%3DNOME_EMPRESA%2BRL%2BLL%2BEBITDA%2BDATA_PUBLICACAO%2BPRECO_ABERTURA%2BPRECO_FECHAMENTO%26periodicidade%3Dtri%26graf_tab%3Dtabela%26desloc_data_analise%3D1%26flag_transpor%3D0%26c_d%3Dd%26enviar_email%3D0%26enviar_email_log%3D0%26cabecalho_excel%3Dmodo1%26relat_alias_automatico%3Dcmd_alias_01&format=json3\"\n",
    "  headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "  response = requests.request(\"POST\", url, data=payload, headers=headers, params=querystring)\n",
    "  data = json.loads(response.text)\n",
    "  df = pd.DataFrame(data[\"tables\"][\"tab0\"]).T\n",
    "  novas_colunas = [\"Data\", \"Empresa\", \"Receita\", \"Lucro\", \"EBITDA\", \"Data_Publicacao\", \"Preco_Abertura\", \"Preco_fechamento\", \"Consolidado\", \"Convencao\", \"Moeda\", \"Data_Demonstracao\", \"Meses\", \"Data_Analise\"]\n",
    "  df.columns = novas_colunas\n",
    "  df = df.drop(\"lin0\")\n",
    "  df['Data_Publicacao'] = pd.to_datetime(df['Data_Publicacao'], errors = 'coerce', format='%d/%m/%Y')\n",
    "  df.reset_index(drop=True, inplace=True)\n",
    "  df['Data_Publicacao'] = pd.to_datetime(df['Data_Publicacao'], format='%d/%m/%Y').dt.strftime('%Y-%m-%d')\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:39:47.095505Z",
     "start_time": "2024-08-29T00:39:03.048464Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYrHaVrZCy3G",
    "outputId": "20c8db17-f3a9-4a71-f193-1bee01f80ffd"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('dataset/prices_processed')\n",
    "for emp in files:\n",
    "  price_p = pd.read_csv(f\"dataset/prices_processed/{emp}\")\n",
    "  date_df = EventsDate(ticker= emp[0:-7])\n",
    "  price_p.insert(1, 'event', price_p['Date'].apply(lambda date: 1 if date in date_df['Data_Publicacao'].values else 0))\n",
    "  price_p.to_csv(f\"dataset/prices_processed/{emp}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T00:25:53.242985Z",
     "start_time": "2024-08-29T00:25:53.224395Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "y4CQ0Anuhz1I",
    "outputId": "2f00dda2-e37a-4feb-f328-5a0572a261de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>event</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily_Return</th>\n",
       "      <th>Week_Return</th>\n",
       "      <th>Month_Return</th>\n",
       "      <th>Cumulative_std_Daily_Return</th>\n",
       "      <th>Cumulative_std_Week_Return</th>\n",
       "      <th>Cumulative_std_Month_Return</th>\n",
       "      <th>Indicator_Daily_Return</th>\n",
       "      <th>Indicator_Week_Return</th>\n",
       "      <th>Indicator_Month_Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>5.455754</td>\n",
       "      <td>7249400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>15.050000</td>\n",
       "      <td>15.050000</td>\n",
       "      <td>14.595000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>5.401014</td>\n",
       "      <td>6129800</td>\n",
       "      <td>-0.010084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0</td>\n",
       "      <td>14.745000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.710000</td>\n",
       "      <td>14.820000</td>\n",
       "      <td>5.408316</td>\n",
       "      <td>5466200</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>14.720000</td>\n",
       "      <td>14.855000</td>\n",
       "      <td>14.705000</td>\n",
       "      <td>14.825000</td>\n",
       "      <td>5.410140</td>\n",
       "      <td>3084400</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>14.910000</td>\n",
       "      <td>14.735000</td>\n",
       "      <td>14.910000</td>\n",
       "      <td>5.441159</td>\n",
       "      <td>4077800</td>\n",
       "      <td>0.005717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>2023-12-21</td>\n",
       "      <td>0</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>26.945000</td>\n",
       "      <td>27.145000</td>\n",
       "      <td>25.773207</td>\n",
       "      <td>20008000</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.024426</td>\n",
       "      <td>0.056844</td>\n",
       "      <td>0.025163</td>\n",
       "      <td>0.061622</td>\n",
       "      <td>0.117950</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>0</td>\n",
       "      <td>27.250000</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>27.035000</td>\n",
       "      <td>27.219999</td>\n",
       "      <td>25.844416</td>\n",
       "      <td>14606400</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.068219</td>\n",
       "      <td>0.025160</td>\n",
       "      <td>0.061614</td>\n",
       "      <td>0.117938</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>0</td>\n",
       "      <td>27.219999</td>\n",
       "      <td>27.480000</td>\n",
       "      <td>27.150000</td>\n",
       "      <td>27.469999</td>\n",
       "      <td>26.081779</td>\n",
       "      <td>7966400</td>\n",
       "      <td>0.009143</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.074024</td>\n",
       "      <td>0.025157</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>0.117927</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>0</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>27.485001</td>\n",
       "      <td>27.290001</td>\n",
       "      <td>27.430000</td>\n",
       "      <td>26.043802</td>\n",
       "      <td>6745000</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.073744</td>\n",
       "      <td>0.025153</td>\n",
       "      <td>0.061596</td>\n",
       "      <td>0.117916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>0</td>\n",
       "      <td>27.430000</td>\n",
       "      <td>27.695000</td>\n",
       "      <td>27.344999</td>\n",
       "      <td>27.695000</td>\n",
       "      <td>26.295410</td>\n",
       "      <td>11680800</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>0.065080</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>0.061587</td>\n",
       "      <td>0.117903</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3473 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  event       Open       High        Low      Close  \\\n",
       "0     2010-01-04      0  14.900000  15.000000  14.850000  14.950000   \n",
       "1     2010-01-05      0  15.050000  15.050000  14.595000  14.800000   \n",
       "2     2010-01-06      0  14.745000  15.000000  14.710000  14.820000   \n",
       "3     2010-01-07      0  14.720000  14.855000  14.705000  14.825000   \n",
       "4     2010-01-08      0  14.850000  14.910000  14.735000  14.910000   \n",
       "...          ...    ...        ...        ...        ...        ...   \n",
       "3468  2023-12-21      0  27.400000  27.400000  26.945000  27.145000   \n",
       "3469  2023-12-22      0  27.250000  27.309999  27.035000  27.219999   \n",
       "3470  2023-12-26      0  27.219999  27.480000  27.150000  27.469999   \n",
       "3471  2023-12-27      0  27.400000  27.485001  27.290001  27.430000   \n",
       "3472  2023-12-28      0  27.430000  27.695000  27.344999  27.695000   \n",
       "\n",
       "      Adj Close    Volume  Daily_Return  Week_Return  Month_Return  \\\n",
       "0      5.455754   7249400           NaN          NaN           NaN   \n",
       "1      5.401014   6129800     -0.010084          NaN           NaN   \n",
       "2      5.408316   5466200      0.001350          NaN           NaN   \n",
       "3      5.410140   3084400      0.000337          NaN           NaN   \n",
       "4      5.441159   4077800      0.005717          NaN           NaN   \n",
       "...         ...       ...           ...          ...           ...   \n",
       "3468  25.773207  20008000      0.001475     0.024426      0.056844   \n",
       "3469  25.844416  14606400      0.002759     0.020600      0.068219   \n",
       "3470  26.081779   7966400      0.009143     0.006025      0.074024   \n",
       "3471  26.043802   6745000     -0.001457     0.003835      0.073744   \n",
       "3472  26.295410  11680800      0.009615     0.009797      0.065080   \n",
       "\n",
       "      Cumulative_std_Daily_Return  Cumulative_std_Week_Return  \\\n",
       "0                             NaN                         NaN   \n",
       "1                             NaN                         NaN   \n",
       "2                        0.008085                         NaN   \n",
       "3                        0.006330                         NaN   \n",
       "4                        0.006696                         NaN   \n",
       "...                           ...                         ...   \n",
       "3468                     0.025163                    0.061622   \n",
       "3469                     0.025160                    0.061614   \n",
       "3470                     0.025157                    0.061605   \n",
       "3471                     0.025153                    0.061596   \n",
       "3472                     0.025150                    0.061587   \n",
       "\n",
       "      Cumulative_std_Month_Return  Indicator_Daily_Return  \\\n",
       "0                             NaN                       0   \n",
       "1                             NaN                       0   \n",
       "2                             NaN                       1   \n",
       "3                             NaN                       0   \n",
       "4                             NaN                       1   \n",
       "...                           ...                     ...   \n",
       "3468                     0.117950                       0   \n",
       "3469                     0.117938                       1   \n",
       "3470                     0.117927                       1   \n",
       "3471                     0.117916                       0   \n",
       "3472                     0.117903                       1   \n",
       "\n",
       "      Indicator_Week_Return  Indicator_Month_Return  \n",
       "0                         0                       0  \n",
       "1                         0                       0  \n",
       "2                         0                       0  \n",
       "3                         0                       0  \n",
       "4                         0                       0  \n",
       "...                     ...                     ...  \n",
       "3468                      1                       1  \n",
       "3469                      1                       1  \n",
       "3470                      0                       1  \n",
       "3471                      0                       1  \n",
       "3472                      1                       1  \n",
       "\n",
       "[3473 rows x 17 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed = pd.read_csv(\"dataset/prices_processed/BBAS3.SA.csv\")\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_processed[df_processed['event']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNnVI9scDHKe"
   },
   "outputs": [],
   "source": [
    "def separate_returns(final_data):\n",
    "    # Inicializar colunas se não existirem\n",
    "    if 'return_daily' not in final_data.columns:\n",
    "        final_data['return_daily'] = np.nan\n",
    "    if 'return_week' not in final_data.columns:\n",
    "        final_data['return_week'] = np.nan\n",
    "    if 'return_month' not in final_data.columns:\n",
    "        final_data['return_month'] = np.nan\n",
    "\n",
    "    first_return_daily_list = []\n",
    "    remaining_return_daily_list = []\n",
    "    first_return_week_list = []\n",
    "    remaining_return_week_list = []\n",
    "    first_return_month_list = []\n",
    "    remaining_return_month_list = []\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    while start_idx < len(final_data):\n",
    "        # Encontra o evento\n",
    "        if 1 in final_data[start_idx:]['event'].values:\n",
    "            event_idx = final_data[start_idx:]['event'].eq(1).idxmax()\n",
    "        else:\n",
    "            break\n",
    "        # Encontrar o próximo evento\n",
    "        if 1 in final_data[event_idx+1:]['event'].values:\n",
    "            prox_event_idx = final_data[event_idx+1:]['event'].eq(1).idxmax()\n",
    "        else:\n",
    "            prox_event_idx = len(final_data)\n",
    "\n",
    "        # Calcular o primeiro retorno diario logo após o evento\n",
    "        if event_idx + 1 < len(final_data):\n",
    "            final_data.loc[event_idx + 1, 'return_daily'] = np.log(final_data.loc[event_idx + 1, 'Close'] / final_data.loc[event_idx, 'Close'])\n",
    "            first_return_daily_list.append(final_data.iloc[event_idx + 1])\n",
    "\n",
    "        # Calcular os retornos diarios restantes até o próximo evento\n",
    "        for i in range(event_idx + 2, prox_event_idx, 1):\n",
    "            final_data.loc[i, 'return_daily'] = np.log(final_data.loc[i, 'Close'] / final_data.loc[i-1, 'Close'])\n",
    "            remaining_return_daily_list.append(final_data.iloc[i])\n",
    "\n",
    "        # Calcular o primeiro retorno semanal logo após o evento\n",
    "        if event_idx + 5 < len(final_data):\n",
    "            final_data.loc[event_idx + 5, 'return_week'] = np.log(final_data.loc[event_idx + 5, 'Close'] / final_data.loc[event_idx, 'Close'])\n",
    "            first_return_week_list.append(final_data.iloc[event_idx + 5])\n",
    "\n",
    "        # Calcular os retornos semanais restantes até o próximo evento\n",
    "        for i in range(event_idx + 10, prox_event_idx, 5):\n",
    "            final_data.loc[i, 'return_week'] = np.log(final_data.loc[i, 'Close'] / final_data.loc[i-5, 'Close'])\n",
    "            remaining_return_week_list.append(final_data.iloc[i])\n",
    "\n",
    "\n",
    "        # Calcular o primeiro retorno mensal logo após o evento\n",
    "        if event_idx + 21 < len(final_data):\n",
    "            final_data.loc[event_idx + 21, 'return_month'] = np.log(final_data.loc[event_idx + 21, 'Close'] / final_data.loc[event_idx, 'Close'])\n",
    "            first_return_month_list.append(final_data.iloc[event_idx + 21])\n",
    "\n",
    "        # Calcular os retornos mensais restantes até o próximo evento\n",
    "        for i in range(event_idx + 42, prox_event_idx, 22):\n",
    "            final_data.loc[i, 'return_month'] = np.log(final_data.loc[i, 'Close'] / final_data.loc[i-21, 'Close'])\n",
    "            remaining_return_month_list.append(final_data.iloc[i])\n",
    "\n",
    "        # Reinicia após o evento\n",
    "        start_idx = event_idx + 1\n",
    "\n",
    "    # Criar DataFrames para o primeiro e os demais retornos semanais e mensais\n",
    "    first_return_daily_df = pd.DataFrame(first_return_daily_list).reset_index()[['index',\n",
    "    'Close', 'Date'\n",
    ", 'event', 'return_daily']]\n",
    "    remaining_return_daily_df = pd.DataFrame(remaining_return_daily_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_daily']]\n",
    "    first_return_week_df = pd.DataFrame(first_return_week_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_week']]\n",
    "    remaining_return_week_df = pd.DataFrame(remaining_return_week_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_week']]\n",
    "    first_return_month_df = pd.DataFrame(first_return_month_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_month']]\n",
    "    remaining_return_month_df = pd.DataFrame(remaining_return_month_list).reset_index()[['index',\n",
    " 'Close', 'Date'\n",
    ", 'event', 'return_month']]\n",
    "\n",
    "    first_return_daily_df.rename(columns={'return_daily': 'return'}, inplace=True)\n",
    "    remaining_return_daily_df.rename(columns={'return_daily': 'return'}, inplace=True)\n",
    "    first_return_week_df.rename(columns={'return_week': 'return'}, inplace=True)\n",
    "    remaining_return_week_df.rename(columns={'return_week': 'return'}, inplace=True)\n",
    "    first_return_month_df.rename(columns={'return_month': 'return'}, inplace=True)\n",
    "    remaining_return_month_df.rename(columns={'return_month': 'return'}, inplace=True)\n",
    "\n",
    "    return first_return_daily_df, remaining_return_daily_df, first_return_week_df, remaining_return_week_df, first_return_month_df, remaining_return_month_df\n",
    "\n",
    "\n",
    "first_return_daily_df, remaining_return_daily_df, first_return_week_df, remaining_return_week_df, first_return_month_df, remaining_return_month_df = separate_returns(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp8C0lHqjyQb"
   },
   "source": [
    "# Análise de Sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSuAMo05moiU"
   },
   "outputs": [],
   "source": [
    "name_prices = {\n",
    "    'azul': 'AZUL4.SA',\n",
    "    'BB': 'BBAS3.SA',\n",
    "    'bradesco': 'BBDC4.SA',\n",
    "    'brf': 'BRFS3.SA',\n",
    "    'ccr': 'CCRO3.SA',\n",
    "    'cosan': 'CSAN3.SA',\n",
    "    'cpfl_energia': 'CPFE3.SA',\n",
    "    'dasa': 'DASA3.SA',\n",
    "    'fleury': 'FLRY3.SA',\n",
    "    'gol': 'GOLL4.SA',\n",
    "    'hapvida': 'HAPV3.SA',\n",
    "    'itau': 'ITUB4.SA',\n",
    "    'locaweb': 'LWSA3.SA',\n",
    "    'magazine_luiza': 'MGLU3.SA',\n",
    "    'mrv_engenharia': 'MRVE3.SA',\n",
    "    'natura': 'NTCO3.SA',\n",
    "    'petrobras': 'PETR4.SA',\n",
    "    'santander': 'SANB11.SA',\n",
    "    'sul_america': 'SULA11.SA',\n",
    "    'totvs': 'TOTS3.SA',\n",
    "    'vale': 'VALE3.SA',\n",
    "    'via_varejo': 'BHIA3.SA'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def contar_indicadores(dataframe1, dataframe2, indicador, tipo=1):\n",
    "    contagem = []\n",
    "    for i, row in dataframe1.iterrows():\n",
    "        inicio = row['data']\n",
    "        fim = row['prox_reuniao']\n",
    "        if not pd.isna(fim):\n",
    "            filtro = dataframe2[(dataframe2['Date'] >= inicio) & (dataframe2['Date'] <= fim) & (dataframe2[f'Indicator_{indicador}'] == tipo)]\n",
    "        else:\n",
    "            fim = inicio + pd.DateOffset(months=3)\n",
    "            filtro = dataframe2[(dataframe2['Date'] >= inicio) & (dataframe2['Date'] <= fim) & (dataframe2[f'Indicator_{indicador}'] == tipo)]\n",
    "        contagem.append(len(filtro))\n",
    "    return contagem\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Abrir o arquivo PDF\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        # Criar um objeto PDF Reader\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Inicializar uma variável para armazenar o texto\n",
    "        all_text = \"\"\n",
    "\n",
    "        # Iterar sobre cada página e extrair o texto\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            all_text += page.extract_text()\n",
    "    # Remover os caracteres de nova linha\n",
    "    all_text = all_text.replace('\\n', ' ')\n",
    "    # Segmentar o texto usando \".\" como separador\n",
    "    segments = all_text.split('. ')\n",
    "\n",
    "    # Remover espaços em branco desnecessários e segmentos vazios\n",
    "    segments = [segment.strip() for segment in segments if segment.strip() and len(segment.strip()) >= 15]\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yQ6z_TimxCm",
    "outputId": "03c1656a-2e7e-4861-cc15-a8a56a2b5147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'magazine_luiza']\n",
      "Skipping file: .ipynb_checkpoints\n",
      "Transcript and returns file already exists for: MGLU3.SA\n"
     ]
    }
   ],
   "source": [
    "file_data_transcripts = os.listdir('dataset/transcripts')\n",
    "print(file_data_transcripts)\n",
    "for file in file_data_transcripts:\n",
    "    if file not in name_prices:\n",
    "        print(f\"Skipping file: {file}\")\n",
    "        continue\n",
    "    if os.path.exists(f'dataset/transcripts_and_returns/{name_prices[file]}.csv'):\n",
    "        print(\"Transcript and returns file already exists for: \" + name_prices[file])\n",
    "        continue\n",
    "    if not os.path.exists(f'dataset/transcripts/{file}/datas.csv'):\n",
    "        print(\"Data file does not exist: \"+file)\n",
    "        continue\n",
    "    df = pd.read_csv(f'dataset/transcripts/{file}/datas.csv')\n",
    "    df = df.sort_values(by=['data'], ignore_index=True)\n",
    "    df['data'] = pd.to_datetime(df['data'])\n",
    "    df['prox_reuniao'] = df['data'].shift(-1)\n",
    "    df['prox_reuniao'] = pd.to_datetime(df['prox_reuniao'])\n",
    "    prices = pd.read_csv(f'dataset/prices_processed/{name_prices[file]}.csv')\n",
    "    prices['Date'] = pd.to_datetime(prices['Date'])\n",
    "    df['Daily_Return_Positive'] = contar_indicadores(df, prices, 'Daily_Return', tipo=1)\n",
    "    df['Daily_Return_Neutral'] = contar_indicadores(df, prices, 'Daily_Return', tipo=0)\n",
    "    df['Daily_Return_Negative'] = contar_indicadores(df, prices, 'Daily_Return', tipo=-1)\n",
    "    df['Daily_Return_Total'] = df['Daily_Return_Positive'] + df['Daily_Return_Neutral'] + df['Daily_Return_Negative']\n",
    "    df['Week_Return_Positive'] = contar_indicadores(df, prices, 'Week_Return', tipo=1)\n",
    "    df['Week_Return_Neutral'] = contar_indicadores(df, prices, 'Week_Return', tipo=0)\n",
    "    df['Week_Return_Negative'] = contar_indicadores(df, prices, 'Week_Return', tipo=-1)\n",
    "    df['Week_Return_Total'] = df['Week_Return_Positive'] + df['Week_Return_Neutral'] + df['Week_Return_Negative']\n",
    "    df['Month_Return_Positive'] = contar_indicadores(df, prices, 'Month_Return', tipo=1)\n",
    "    df['Month_Return_Neutral'] = contar_indicadores(df, prices, 'Month_Return', tipo=0)\n",
    "    df['Month_Return_Negative'] = contar_indicadores(df, prices, 'Month_Return', tipo=-1)\n",
    "    df['Month_Return_Total'] = df['Month_Return_Positive'] + df['Month_Return_Neutral'] + df['Month_Return_Negative']\n",
    "    df.to_csv(f'dataset/transcripts_and_returns/{name_prices[file]}.csv', index=False)\n",
    "    print(f\"Transcript and return created in: {name_prices[file]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xGPZnxOjtZY",
    "outputId": "0aba62b4-db50-42cd-d65e-d924e830a81b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred_mapper = {\n",
    "    0: 1,\n",
    "    1: -1,\n",
    "    2: 0\n",
    "  }\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")\n",
    "finbertptbr = BertForSequenceClassification.from_pretrained(\"lucas-leme/FinBERT-PT-BR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGXlCKUJjw9L",
    "outputId": "0dd3f26d-8565-4130-e827-8b019ae4762d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocorreu um erro: name 'pd' is not defined\n"
     ]
    }
   ],
   "source": [
    "files = ['magazine_luiza']  # os.listdir(\"../../dataset/transcripts\")\n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_csv(f\"dataset/transcripts_and_returns/{name_prices[file]}.csv\")\n",
    "        for i, row in df.iterrows():\n",
    "            # if not pd.isna(row['positive_sentiment']):\n",
    "            #     print(row['positive_sentiment'])\n",
    "            #     continue\n",
    "            text_extract_name = row['trimestre']\n",
    "            text = extract_text_from_pdf(f'dataset/transcripts/{file}/{text_extract_name}.pdf')\n",
    "            tokens = tokenizer(text, return_tensors=\"pt\",\n",
    "                                   padding=True, truncation=True, max_length=512)\n",
    "\n",
    "            finbertptbr_outputs = finbertptbr(**tokens)\n",
    "            preds = [pred_mapper[np.argmax(pred)] for pred in finbertptbr_outputs.logits.cpu().detach().numpy()]\n",
    "            posi, neut, nega = preds.count(1), preds.count(0), preds.count(-1)\n",
    "            df.loc[i, 'positive_sentiment'] = posi\n",
    "            df.loc[i, 'neutral_sentiment'] = neut\n",
    "            df.loc[i, 'negative_sentiment'] = nega\n",
    "            df.to_csv(f\"dataset/transcripts_and_returns/{name_prices[file]}.csv\", index=False)\n",
    "\n",
    "            del tokens, finbertptbr_outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro de arquivo não encontrado: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwYIZBn8Tw0u"
   },
   "source": [
    "# Previsão de Preços"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "87IYGmTRUALm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcOJSna7Twf5",
    "outputId": "127b21b9-6431-4307-c994-e602783ce8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro Quadrático Médio: 0.8151249761458873\n",
      "Preço previsto para 2024-08-11: 17.2836993598938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Criar um DataFrame\n",
    "df = pd.read_csv('dataset/prices/AZUL4.SA.csv')\n",
    "\n",
    "# Converter a coluna 'Date' para o formato datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extrair recursos da data (Ano, Mês, Dia) para usar como variáveis independentes\n",
    "df['Ano'] = df['Date'].dt.year\n",
    "df['Mes'] = df['Date'].dt.month\n",
    "df['Dia'] = df['Date'].dt.day\n",
    "\n",
    "# Definir variáveis independentes (X) e dependentes (y)\n",
    "X = df[['Ano', 'Mes', 'Dia']]  # variáveis independentes\n",
    "y = df['Close']  # variável dependente\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar o modelo Random Forest\n",
    "modelo = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo usando o erro quadrático médio (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Erro Quadrático Médio: {mse}')\n",
    "\n",
    "# Prever o preço para uma nova data (ex: 2024-08-11)\n",
    "nova_data = pd.to_datetime('2024-08-11')\n",
    "ano = nova_data.year\n",
    "mes = nova_data.month\n",
    "dia = nova_data.day\n",
    "\n",
    "novo_dia = np.array([[ano, mes, dia]])\n",
    "preco_previsto = modelo.predict(novo_dia)\n",
    "print(f'Preço previsto para 2024-08-11: {preco_previsto[0]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
